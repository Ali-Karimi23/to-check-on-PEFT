{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a47574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Mount Google Drive if in Colab\n",
    "import os\n",
    "if os.path.isdir('/content/MyDrive'):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/MyDrive')\n",
    "    BASE_DIR = 'content/MyDrive/00-github/to-check-on-PEFT/to-check-on-PEFT'\n",
    "else:\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(__file__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Import Libraries\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "from peft import VBLoRAConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bdbba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title names and parameters\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'Models')\n",
    "MODEL_NAME = 'bert-base-uncased'  # @param {type:\"string\"}\n",
    "DATASET_NAME = 'glue'  # @param {type:\"string\"}\n",
    "DATASET_CONFIG_NAME = 'mrpc'  # @param [\"mrpc\", \"sst2\", \"cola\", \"qnli\", \"qqp\", \"stsb\", \"mnli\", \"rte\", \"wnli\"]\n",
    "NUM_LABELS = 2  # @param {type:\"integer\"}\n",
    "TASK_TYPE = 'SEQ_CLS'  # @param [\"CAUSAL_LM\",\"SEQ_2_SEQ_LM\",\"SEQ_CLS\",\"TOKEN_CLS\",\"QUESTION_ANS\",\"FEATURE_EXTRACTION\",\"MULTIPLE_CHOICE\",\"IMAGE_CLASSIFICATION\",\"AUDIO_CLS\"]\n",
    "BATCH_SIZE = 16  # @param {type:\"integer\"}\n",
    "EPOCHS = 3  # @param {type:\"integer\"}\n",
    "LEARNING_RATE = 2e-5  # @param {type:\"number\"}\n",
    "LORA_RANK = 8  # @param {type:\"integer\"}\n",
    "LORA_ALPHA = 16  # @param {type:\"integer\"}\n",
    "LORA_DROPOUT = 0.1  # @param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074342d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Model and Tokenizer Setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9069fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Prepare VB-LoRA Configuration\n",
    "peft_config = VBLoRAConfig(\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"query\", \"value\", \"key\", \"dense\"], # Adjust based on model architecture \n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea029923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Load GLUE tasks\n",
    "dataset = load_dataset(DATASET_NAME, DATASET_CONFIG_NAME)\n",
    "metric = load_metric('glue', DATASET_CONFIG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a847b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Tokenization and Preprocessing\n",
    "def preprocess_fn(example):\n",
    "    # for binary classification tasks\n",
    "    return tokenizer(example[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "encoded = dataset[\"train\"].map(preprocess_fn, batched=False)\n",
    "encoded_eval = dataset[\"validation\"].map(preprocess_fn, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d92b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Trainer and Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# define computing function\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = logits.argmax(-1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded,\n",
    "    eval_dataset=encoded_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Save VB-LoRA Adapter\n",
    "adapter_path = os.path.join(MODEL_DIR, \"vblora_adapter_\" + TASK_TYPE)\n",
    "os.makedirs(adapter_path, exist_ok=True)\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(\"Saved VB-LoRA adapter at:\", adapter_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentence_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
